---
---

@string{ICCV={Proc. ICCV}}
@string{WACV={Proc. WACV}}
@string{SGP={Proc. SGP}}
@string{CGF_SGP={Computer Graphics Forum (Proc. SGP)}}

%%% 2024

@inproceedings{Georgiou:2023:FacadeNet,
  author    = {Georgiou, Y. and Loizou, M. and Kelly, T. and Averkiou, M.},
  title     = {FacadeNet: Conditional Facade Synthesis via Selective Editing},
  booktitle	= WACV,
  year      = {2024},
  abstract  = {We introduce FacadeNet, a deep learning approach for synthesizing building 
			   facade images from diverse viewpoints. Our method employs a conditional GAN, taking a
			   single view of a facade along with the desired viewpoint information and 
			   generates an image of the facade from the distinct viewpoint. To 
			   precisely modify view-dependent elements like windows and doors while 
			   preserving the structure of view-independent components such as walls, 
			   we introduce a selective editing module. This module leverages image 
			   embeddings extracted from a pretrained vision transformer. Our 
			   experiments demonstrated state-of-the-art performance on building 
			   facade generation, surpassing alternative methods},
  img		= {../assets/img/facadenet.png},
  selected  = False
}

%%% 2023

@article{Loizou:2023:CSN,
  author    = {Loizou, M. and Garg, S. and Petrov, D. and Averkiou, M. and Kalogerakis, E.},
  title     = {Cross-Shape Attention for Part Segmentation of 3D Point Clouds},
  journal	= CGF_SGP,
  year      = {2023},
  abstract  = {We present a deep learning method that propagates point-wise feature representations across shapes within
               a collection for the purpose of 3D shape segmentation. We propose a cross-shape attention mechanism to
               enable interactions between a shape's point-wise features and those of other shapes. The mechanism assesses
               both the degree of interaction between points and also mediates feature propagation across shapes, improving
               the accuracy and consistency of the resulting point-wise feature representations for shape segmentation.
               Our method also proposes a shape retrieval measure to select suitable shapes for cross-shape attention
               operations for each test shape. Our experiments demonstrate that our approach yields state-of-the-art
               results in the popular PartNet dataset.},
  volume    = {42},
  number    = {5},
  website   = {https://marios2019.github.io/CSN/},
  code      = {https://github.com/marios2019/CSN},
  pdf		= {https://arxiv.org/pdf/2003.09053.pdf},
  img		= {../assets/img/CSN.jpeg},
  selected  = True
}

%%% 2022

@inproceedings{Sharma:2022:PriFit,
  author    = {Sharma, G. and Dash, B. and RoyChowdhury, A. and Gadelha, M. and Loizou, M.
			   and Cao, L. and Wang, R and Learned-Miller, E. G. and Maji, S and Kalogerakis, E.},
  title     = {PRIFIT: Learning to Fit Primitives Improves Few Shot Point Cloud Segmentation},
  booktitle	= SGP,
  year      = {2022},
  abstract  = {We present PriFit, a simple approach for label efficient learning of 3D shape segmentation networks.
	           PriFit is based on a self-supervised task of decomposing the surface of a 3D shape into geometric primitives.
			   It can be readily applied to existing network architectures for 3D shape segmentation, and improves their
			   performance in the few-shot setting, as we demonstrate in the widely used ShapeNet and PartNet benchmarks.
			   PriFit outperforms the prior state-of-the-art in this setting, suggesting that decomposability into primitives
			   is a useful prior for learning representations predictive of semantic parts. We present a number of
			   experiments varying the choice of geometric primitives and downstream tasks to demonstrate the effectiveness
			   of the method.},
  website   = {https://hippogriff.github.io/prifit/},
  code      = {https://github.com/Hippogriff/prifit},
  pdf		= {https://arxiv.org/pdf/2112.13942.pdf},
  img		= {../assets/img/prifit.jpg},
  selected  = True
}

%%% 2021

@inproceedings{Selvaraju:2021:BuildingNet, 
  author    = {Selvaraju, P. and Nabail, M. and Loizou, M. and Maslioukova, M. and
               Averkiou, M. and Andreou, A. and Chaudhuri, S. and Kalogerakis, E.},
  title     = {BuildingNet: Learning to Label 3D Buildings},
  booktitle = ICCV,
  year      = {2021},
  abstract  = {We introduce BuildingNet: (a) a large-scale dataset of
			   3D building models whose exteriors are consistently labeled,
			   and (b) a graph neural network that labels building meshes
			   by analyzing spatial and structural relations of their geometric
			   primitives. To create our dataset, we used crowdsourcing combined
			   with expert guidance, resulting in 513K annotated mesh primitives,
			   grouped into 292K semantic part components across 2K building models.
			   The dataset covers several building categories, such as houses, 
			   churches, skyscrapers, town halls, libraries, and castles. We include 
			   a benchmark for evaluating mesh and point cloud labeling. Buildings
			   have more challenging structural complexity compared to objects in
			   existing benchmarks (e.g., ShapeNet, PartNet), thus, we hope that 
			   our dataset can nurture the development of algorithms that are able 
			   to cope with such large-scale geometric data for both vision and
			   graphics tasks e.g., 3D semantic segmentation, part-based generative
			   models, correspondences, texturing, and analysis of point cloud data
			   acquired from real-world buildings. Finally, we show that our mesh-based
			   graph neural network significantly improves performance over several
			   baselines for labeling 3D meshes. Our project page www.buildingnet.org
			   includes our dataset and code.},
  website   = {https://buildingnet.org/},
  code      = {https://github.com/buildingnet/buildingnet_dataset},
  pdf		= {https://arxiv.org/pdf/2110.04955.pdf},
  img		= {../assets/img/buildingnet.jpg},
  video     = {https://www.youtube.com/watch?v=rl30WJo_EBo&ab_channel=EvangelosKalogerakis},
  selected  = True
}

%%% 2020

@article{Loizou:2021:PB-DGCNN:,
  author    = {Loizou, M. and Averkiou, M. and Kalogerakis, E},
  title     = {Learning Part Boundaries from 3D Point Clouds},
  journal   = CGF_SGP,	
  year      = {2020},
  abstract  = {We present a method that detects boundaries of parts in 3D shapes represented as 
			   point clouds. Our method is based on a graph convolutional network architecture that
			   outputs a probability for a point to lie in an area that separates two or more parts
			   in a 3D shape. Our boundary detector is quite generic: it can be trained to localize
			   boundaries of semantic parts or geometric primitives commonly used in 3D modeling.
			   Our experiments demonstrate that our method can extract more accurate boundaries that
			   are closer to ground-truth ones compared to alternatives. We also demonstrate an
			   application of our network to fine-grained semantic shape segmentation, where we also
			   show improvements in terms of part labeling performance.},
  volume    = {39},
  number    = {5},
  website   = {https://marios2019.github.io/learning_part_boundaries/},
  code      = {https://github.com/marios2019/learning_part_boundaries},
  pdf		= {https://arxiv.org/pdf/2007.07563.pdf},
  img       = {../assets/img/pb_dgcnn.jpg},
  video		= {https://www.youtube.com/watch?v=pyCZiK28nl0&ab_channel=MariosLoizou},
  selected  = True
}

%%% 2019

@inproceedings{Loizou:2019:VisualTracking, 
  author    = {Loizou, M. and Kaimakis, P.},
  title     = {Model-Based 3D Visual Tracking of Rigid Bodies using Distance Transform},
  booktitle = {Proc. VISUAL},
  year      = {2019},
  abstract  = {The core idea of model-based 3D tracking is that of continuously estimating
			   the pose parameters of a 3D object throughout a sequence of images, e.g., 
			   a video feed. Here, we present an edge-based method for achieving 3D object
			   tracking, via Gauss-Newton optimization. We rely on natural features observations,
			   like edges, for the detection of interest points and by using the 3D pose of the
			   object in the previous frame, we correctly estimate its new 3D position and 
			   orientation, in real-time. There is also a C++ implementation of the visual tracking system,
			   with the use of the OpenCV library, which can be found in our GitHub repository 
			   (https://github.com/marios2019/Visual Tracking).},
  code      = {https://github.com/marios2019/Visual_Tracking},
  pdf		= {https://www.researchgate.net/profile/William-Hurst-4/publication/340897785_The_Fourth_International_Conference_on_Applications_and_Systems_of_Visual_Paradigms_-_2019/links/5ea2e33e458515ec3a0306ed/The-Fourth-International-Conference-on-Applications-and-Systems-of-Visual-Paradigms-2019.pdf#page=49},
  img		= {../assets/img/visual_tracking.JPG},
  selected  = False
} 